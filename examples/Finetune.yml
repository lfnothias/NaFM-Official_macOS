# training settings
num_epochs: 300
lr: 1.e-4
lr_min: 1.e-6
weight_decay: 1.e-5
early_stopping_patience: 300 # 100

# dataset specific
dataset_root: ./downstream_data/Ontology
mask_ratio: 0.
dataset: Ontology
dataset_arg: Class
confine_training: true
num_train_samples: 4

# dataloader specific
reload: 0
batch_size: 256
inference_batch_size: 512
splits: null
train_size: null
val_size: 0.1
test_size: 0.1
num_workers: 12

# architectural specific
emb_dim: 2048
num_layer: 6
feat_dim: 1024
feat_dim: null
drop_ratio: 0.1
linear_drop_ratio: 0.3
temperature: null
pretrained_path: epoch=84-val_loss=1.6483.ckpt
gnn_type: gin
freeze: false

# other specific
ngpus: -1
num_nodes: 1
precision: 32
log_dir: ./logs-finetune-tune/seed_0
seed: 0
distributed_backend: ddp
accelerator: gpu
save_interval: 5
task: finetune
